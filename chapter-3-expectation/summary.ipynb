{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecc9e16",
   "metadata": {},
   "source": [
    "# Expectation\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The \"expected value\"/\"mean\"/\"first moment\" of $X$ is defined as:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{E}(X) = \\mu_X = \\sum_x xf_X(x)$, where $\\mathbb{E}(|X|) < \\infty \\iff \\sum_x |x|f_X(x) < \\infty$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{E}(X) = \\mu_X = \\int xdF_X(x) = \\int xf_X(x)dx$, where $\\mathbb{E}(|X|) < \\infty \\iff \\int |x|f_X(x)dx < \\infty$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (the rule of lazy statistician)**\n",
    "\n",
    "Let $Y = r(X)$. Then:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{E}(Y) = \\mathbb{E}(r(X)) = \\sum_x r(x)f_X(x)$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{E}(Y) = \\mathbb{E}(r(X)) = \\int r(x)dF_X(x) = \\int r(x)f_X(x)dx$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (the rule of lazy statistician extended)**\n",
    "\n",
    "Let $Z = r(X, Y)$. Then:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{E}(Z) = \\mathbb{E}(r(X, Y)) = \\sum_{x,y} r(x,y)f_{XY}(x,y)$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{E}(Z) = \\mathbb{E}(r(X, Y)) = \\int \\int r(x,y)dF_{XY}(x,y) = \\int \\int r(x,y)f_{XY}(x,y)dxdy$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The $k^{th}$ moment of $X$ is $\\mathbb{E}(X^k)$, where $\\mathbb{E}(|X^k|) < \\infty$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "If $k^{th}$ moment exists ($\\mathbb{E}(|X^k|) < \\infty$) and if $j < k$, then $j^{th}$ moment exists.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The $k^{th}$ central moment of $X$ is $\\mathbb{E}((X-\\mathbb{E}(X))^k) = \\mathbb{E}((X-\\mu_X)^k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae84a4",
   "metadata": {},
   "source": [
    "### Properties of Expectations\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "If $X_1,...,X_n$ are random variables and $a_1,...,a_n$ are constants. Then:\n",
    "\n",
    "$\\mathbb{E}(\\sum_i a_i X_i) = \\sum_i a_i\\mathbb{E}(X_i)$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "Let $X_1,...,X_n$ be independent random variables. Then:\n",
    "\n",
    "$\\mathbb{E}(\\prod_i X_i) = \\prod_i \\mathbb{E}(X_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681bbab",
   "metadata": {},
   "source": [
    "### Variance and Covariance\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Let $X$ be a random variable with mean $\\mu$. The variance of $X$ is defined by:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{V}(X) = \\sigma_X^2 = \\mathbb{E}((X - \\mu)^2) = \\sum (x - \\mu)^2 f_X(x)$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{V}(X) = \\sigma_X^2 = \\mathbb{E}((X - \\mu)^2) = \\int (x - \\mu)^2 dF_X(x) = \\int (x - \\mu)^2 f_X(x)dx$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The standard deviation of $X$ is:\n",
    "\n",
    "$\\sigma_X = \\sqrt{\\mathbb{V}(X)}$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "Variance has the following properties:\n",
    "\n",
    "- $\\mathbb{V}(X) = \\mathbb{E}(X^2) - \\mu^2$\n",
    "\n",
    "- If $a$ and $b$ are constants, then $\\mathbb{V}(aX + b) = a^2\\mathbb{V}(X)$\n",
    "\n",
    "- If $X_1,...,X_n$ are independent and $a_1,...,a_n$ are constants, then $\\mathbb{V}(\\sum_i a_i X_i) = \\sum_i a_i^2 \\mathbb{V}(X_i)$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Let $X$ and $Y$ be random variables with means $\\mu_X$ and $\\mu_Y$, and standard deviations $\\sigma_X$ and $\\sigma_Y$. The covariance between $X$ and $Y$ is defined as:\n",
    "\n",
    "$Cov(X,Y) = \\mathbb{E}((X-\\mu_X)(Y-\\mu_Y))$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Let $X$ and $Y$ be random variables with means $\\mu_X$ and $\\mu_Y$, and standard deviations $\\sigma_X$ and $\\sigma_Y$. The correlation between $X$ and $Y$ is defined as:\n",
    "\n",
    "$\\rho = \\rho_{XY} = \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "The covariance satisifies:\n",
    "\n",
    "$Cov(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "The correlation satisfies:\n",
    "\n",
    "$-1\\leq \\rho(X,Y)\\leq 1$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "If $Y = aX + b$, then $\\rho(X,Y) = 1$ if $a > 0$ and $\\rho(X,Y) = -1$ if $a < 0$. The converse is not true in general.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "If $X$ and $Y$ are independent, then $Cov(X,Y) = \\rho(X,Y) = 0$. The converse is not true in general.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "$\\mathbb{V}(X+Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) + 2Cov(X,Y)$\n",
    "\n",
    "$\\mathbb{V}(X-Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) - 2Cov(X,Y)$\n",
    "\n",
    "For random variables $X_1,...,X_n$:\n",
    "\n",
    "$\\mathbb{V}(\\sum_i a_i X_i) = \\sum_i a_i^2 \\mathbb{V}(X_i) + 2\\sum\\sum_{i<j} a_i a_j Cov(X_i, X_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1d6fc",
   "metadata": {},
   "source": [
    "### Sample mean and variance\n",
    "\n",
    "**Definition**\n",
    "\n",
    "If $X_1,...,X_n$, the sample mean is defined by:\n",
    "\n",
    "$\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "If $X_1,...,X_n$, the sample variance is defined by:\n",
    "\n",
    "$S_n^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X}_n)^2$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "Let $X_1,...,X_n$ be IID and let $\\mu = \\mathbb{E}(X_i)$ and $\\sigma^2 = \\mathbb{V}(X_i)$. Then:\n",
    "\n",
    "- $\\mathbb{E}(\\overline{X}_n) = \\mu$\n",
    "\n",
    "- $\\mathbb{V}(\\overline{X}_n) = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "- $\\mathbb{E}(S_n^2) = \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd87e57",
   "metadata": {},
   "source": [
    "### Multivariate Mean and Variance\n",
    "\n",
    "**Definition**\n",
    "\n",
    "If $X = \\begin{bmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{bmatrix}$, then $\\mathbb{E}(X) = \\begin{bmatrix} \\mathbb{E}(X_1) \\\\ \\vdots \\\\ \\mathbb{E}(X_n) \\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "If $X = \\begin{bmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{bmatrix}$, then $\\mathbb{V}(X) = \n",
    "\\begin{bmatrix}\n",
    "\\mathbb{V}(X_1) & Cov(X_1, X_2) & \\cdots & Cov(X_1, X_n) \\\\\n",
    "Cov(X_2, X_1) & \\mathbb{V}(X_2) & \\cdots & Cov(X_2, X_n) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "Cov(X_n, X_1) & Cov(X_n, X_2) & \\cdots & \\mathbb{V}(X_n)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "If $a$ is a vector and $X$ is a random vector with mean $\\mu$ and variance $\\Sigma$, then $\\mathbb{E}(a^T X) = a^T\\mu$ and $\\mathbb{V}(a^T X) = a^T\\Sigma a$. If $A$ is a matrix, then $\\mathbb{E}(AX) = A\\mu$ and $\\mathbb{V}(AX) = A\\Sigma A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5e086",
   "metadata": {},
   "source": [
    "### Conditional Expectation\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The conditional expectation of $X$ given $Y=y$ is:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{E}(X|Y=y) = \\sum x f_{X|Y}(x|y)$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{E}(X|Y=y) = \\int x f_{X|Y}(x|y)dx$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "If $r(x,y)$ is a function of $x$ and $y$, the conditional expectation of $r(X,Y)$ given $Y=y$ is:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{E}(r(X,Y)|Y=y) = \\sum r(x,y) f_{X|Y}(x|y)$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{E}(r(X,Y)|Y=y) = \\int r(x,y) f_{X|Y}(x|y)dx$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (the rule of iterated expectations)**\n",
    "\n",
    "For random variables $X$ and $Y$, assuming the expectations exist, we have that:\n",
    "\n",
    "$\\mathbb{E}(\\mathbb{E}(Y|X)) = \\mathbb{E}(Y)$ and $\\mathbb{E}(\\mathbb{E}(X|Y)) = \\mathbb{E}(X)$\n",
    "\n",
    "More generally, for any function $r(x,y)$, we have that:\n",
    "\n",
    "$\\mathbb{E}(\\mathbb{E}(r(X,Y)|X)) = \\mathbb{E}(r(X,Y))$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The conditional variance is defined as:\n",
    "\n",
    "- Discrete case\n",
    "\n",
    "$\\mathbb{V}(Y|X=x) = \\sum (y - \\mathbb{E}(Y|X=x))^2 f(y|x)$\n",
    "\n",
    "- Continuous case\n",
    "\n",
    "$\\mathbb{V}(Y|X=x) = \\int (y - \\mathbb{E}(Y|X=x))^2 f(y|x)dy$\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "For random variables $X$ and $Y$,\n",
    "\n",
    "$\\mathbb{V}(Y) = \\mathbb{E}(\\mathbb{V}(Y|X)) + \\mathbb{V}(\\mathbb{E}(Y|X))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589edac",
   "metadata": {},
   "source": [
    "### Moment Generating Functions\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The moment generating function (MGF) of $X$ (Laplace transform of $X$) is defined by:\n",
    "\n",
    "$\\psi_X(t) = \\mathbb{E}(e^{tX}) = \\int e^{tx}dF_X(x)$, where $t\\in\\mathbb{R}$\n",
    "\n",
    "- $\\psi'(0) = \\left[\\frac{d}{dt}\\mathbb{E}(e^{tX})\\right]_{t=0} = \\left[\\mathbb{E}(\\frac{d}{dt}e^{tX})\\right]_{t=0} = \\left[\\mathbb{E}(Xe^{tX})\\right]_{t=0} = \\mathbb{E}(X)$\n",
    "\n",
    "- $\\psi^{(k)}(0) = \\mathbb{E}(X^k)$\n",
    "\n",
    "---\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "Properties of the MGF:\n",
    "\n",
    "- if $Y=aX+b$, then $\\psi_Y(t) = e^{bt}\\psi_X(at)$\n",
    "\n",
    "- if $X_1,...,X_n$ are independent and $Y = \\sum_i X_i$, then $\\psi_Y(t) = \\prod_i \\psi_i(t)$, where $\\psi_i$ is the MGF of $X_i$.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem**\n",
    "\n",
    "Let $X$ and $Y$ be random variables. If $\\psi_X(t) = \\psi_Y(t)$ for all $t$ in an open interval around 0, then $X$ and $Y$ are equal in distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
